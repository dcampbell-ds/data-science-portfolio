{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "b1fc3f9f",
      "metadata": {
        "id": "b1fc3f9f"
      },
      "source": [
        "# DAT 402 Project 1\n",
        "- Marisa Thomas & Dylan Campbell"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd82859e",
      "metadata": {
        "id": "fd82859e"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score,classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c81f1052",
      "metadata": {
        "id": "c81f1052"
      },
      "source": [
        "## ðŸˆ NFL Playoff Prediction: Dataset ðŸˆ\n",
        "- **Dataset Name:** NFL Team-Level Stats (2018-2023)\n",
        "- **Source:** [Pro Football Reference](https://www.pro-football-reference.com/)\n",
        "- **Target Variable:** `MadePlayoffs`\n",
        "- The dataset includes a variety of regular season performance metrics that may influence whether a team qualifies for the playoffs.\n",
        "\n",
        "\n",
        "To build this dataset, we manually compiled and cleaned multiple sources of indivdual team data from [Pro Football Reference](https://www.pro-football-reference.com/) for each season from 2018 to 2023. This involved:\n",
        "- Regular season standings (wins, losses, point differntials, etc.)\n",
        "- Advanced team statistics (SRS, strength of schedule, yards per play, etc.)\n",
        "- Offesnsive and defensive performance metrics\n",
        "\n",
        "Each season's data was collected separately and then merged into one cohesive CSV file for analysis. This required aligning team names, engineering consistent feature formats, and creating a target variable (`MadePlayoffs`) to indicate postseason qualification.\n",
        "\n",
        "---\n",
        "\n",
        "### The dataset contains features such as:\n",
        "- `PointsScored`\n",
        "- `PointsAllowed`\n",
        "- `YardsPerPlay`\n",
        "- `Turnovers`\n",
        "- `RushingYards` and `PassingYards`\n",
        "- `SimpleRatingSystem` (SRS)\n",
        "- `StrengthOfSchedule` (SoS)\n",
        "- `Penalties` and `PenaltyYards`\n",
        "- `ScoringDrive%` and `TurnoverDrive%`\n",
        "- `Season` (2018-2023)\n",
        "- `Team` (name)\n",
        "\n",
        "Each row represents one NFL team in one season, with all stats coming from the regular season only. The target column `MadePlayoffs` is a binary variable where 1 means the team made the playoffs, and 0 means they did not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e2d54ca",
      "metadata": {
        "id": "6e2d54ca"
      },
      "outputs": [],
      "source": [
        "# Loading data\n",
        "df = pd.read_csv(\"NFL_Data.csv\")\n",
        "target = ['MadePlayoffs']\n",
        "\n",
        "display(df.head(4))\n",
        "print(\"df shape:\", df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf30abf6",
      "metadata": {
        "id": "cf30abf6"
      },
      "outputs": [],
      "source": [
        "#check for missing values in the data\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c33c7024",
      "metadata": {
        "id": "c33c7024"
      },
      "source": [
        "### Merging and Cleaning the Data\n",
        "Before building our model, we first collected and cleaned raw data from two separate CSV files:\n",
        "- `AFC Regular Season Stats 2018-2023.csv`\n",
        "- `NFC Regular Season Stats 2018-2023.csv`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "442a21b9",
      "metadata": {
        "id": "442a21b9"
      },
      "outputs": [],
      "source": [
        "#Load both CSVs\n",
        "afc_df = pd.read_csv(\"AFC Regular Season Stats 2018-2023.csv\")\n",
        "nfc_df = pd.read_csv(\"NFC Regular Season Stats 2018-2023.csv\")\n",
        "\n",
        "display(afc_df.head(4))\n",
        "display(nfc_df.head(4))\n",
        "\n",
        "print(\"AFC shape:\", afc_df.shape)\n",
        "print(\"NFC Shape:\", nfc_df.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0b43ef2",
      "metadata": {
        "id": "f0b43ef2"
      },
      "source": [
        "After this, we had to build a complete and clean dataset from our two csv files.\n",
        "\n",
        "### Steps:\n",
        "- Dropped irrelevant or non-numeric columns such as Opp, Home/Away,and raw date labels.\n",
        "- Renamed columns for clarity - e.g., `Yds.1` --> `SackYardsLost`,  `Pts` --> `PointsScored`, etc.\n",
        "- Converted columns like `Date` To actual `datetime` objects and used them to assign a `Season` column.\n",
        "- Created a binary column `Win` where each game result was encoded (1 = Win, 0 = Loss/Tie).\n",
        "- Aggregated all team-level stats, summing most stats and averaging others.\n",
        "- Concatenated cleaned AFC and NFC datasets into a single dataframe.\n",
        "- Merged with the cleaned standings data to bring in `TeamRating`, `StrengthOfSchedule`, `OffenseRating`, and `DefenseRating`.\n",
        "- Performed final column cleaning (removing `*` and `+` from team names) and exported the resulting dataset as:\n",
        "    - `NFL_Data.csv`\n",
        "\n",
        "\n",
        "Below is the code for reference. (commented out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37645525",
      "metadata": {
        "id": "37645525"
      },
      "outputs": [],
      "source": [
        "# nfc = pd.read_csv(\"NFC_Regular_Season_2018-2023.csv\", skiprows=1) ##Removes blank row in header\n",
        "\n",
        "# ##Rename columns to make data fram easier to understand. Yds.1, Yds.2, ... main reasons for adjustment.\n",
        "# nfc.rename(columns={\n",
        "#   'Tm': 'Team',\n",
        "#   'Rk': 'Rank',\n",
        "#   'Gtm': 'GameNumber',\n",
        "#   'Pts': 'PointsScored',\n",
        "#   'PtsO': 'PointsAllowed',\n",
        "#   'Cmp': 'PassesCompleted',\n",
        "#   'Att': 'PassesAttempted',\n",
        "#   'Cmp%': 'PercentageCompleted',\n",
        "#   'Yds': 'PassingYards',\n",
        "#   'TD': 'PassingTDs',\n",
        "#   'Y/A': 'PassingYardsPerAttempt',\n",
        "#   'AY/A': 'AdjustedPassingYardsPerAttempt',\n",
        "#   'Rate': 'PasserRating',\n",
        "#   'Sk': 'SacksTaken',\n",
        "#   'Yds.1': 'SackYardsLost',\n",
        "#   'Att.1': 'RushingAttempts',\n",
        "#   'Yds.2': 'RushingYards',\n",
        "#   'TD.1': 'RushingTDs',\n",
        "#   'Y/A.1': 'RushingYardsPerAttempt',\n",
        "#   'Ply': 'OffensivePlays',\n",
        "#   'Tot': 'TotalYards',\n",
        "#   'Y/P': 'YardsPerPlay',\n",
        "#   'FGA': 'FieldGoalAttempts',\n",
        "#   'FGM': 'FieldGoalsMade',\n",
        "#   'XPA': 'ExtraPointAttempts',\n",
        "#   'XPM': 'ExtraPointsMade',\n",
        "#   'Pnt': 'TimesPunted',\n",
        "#   'Yds.3': 'PuntYards',\n",
        "#   'Pass': 'FirstDownsByPassing',\n",
        "#   'Rsh': 'FirstDownsByRushing',\n",
        "#   'Pen': 'FirstDownsByPenalty',\n",
        "#   '1stD': 'FirstDowns',\n",
        "#   '3DConv': 'ThirdDownConversions',\n",
        "#   '3DAtt': 'ThirdDownAttempts',\n",
        "#   '4DConv': 'FourthDownConversions',\n",
        "#   '4DAtt': 'FourthDownAttempts',\n",
        "#   'Pen.1': 'PenaltiesCommitted',\n",
        "#   'Yds.4': 'PenaltyYards',\n",
        "#   'FL': 'FumblesLost',\n",
        "#   'Int': 'InterceptionsThrown',\n",
        "#   'TO': 'TurnoversLost',\n",
        "#   'ToP': 'TimeOfPossession'\n",
        "# }, inplace=True)\n",
        "\n",
        "# nfc_updated = nfc.copy() ##Work is being done on afc_updated through a copy of nfc df\n",
        "# nfc_updated['Win'] = nfc_updated['Rslt'].apply(lambda x: 1 if x == 'W' else 0) ##changed rslt to win column and represented with binary (win = 1, loss, tie = 0)\n",
        "# nfc_updated['Date'] = pd.to_datetime(nfc_updated['Date']) ##convert to datetime object\n",
        "# nfc_updated['Season'] = nfc_updated['Date'].apply(lambda x: x.year - 1 if x.month < 2 else x.year) ##create 'Season' column based on datetime object\n",
        "# nfc_updated.drop(columns=['Rank', 'Date', 'Div', 'GameNumber', 'Home/Away', 'Opp', 'Rslt', 'Week', 'Day', 'OT', 'PassesCompleted', 'PassesAttempted', 'PassingYardsPerAttempt', 'AdjustedPassingYardsPerAttempt', 'RushingYardsPerAttempt', 'FirstDownsByPassing', 'FirstDownsByRushing', 'FirstDownsByPenalty', 'TimeOfPossession'], inplace=True)\n",
        "# ##dropped columns that are not numeric or those that are expected to be non-impactful compared to the remaining.\n",
        "\n",
        "# print(nfc_updated.columns)\n",
        "# print(nfc_updated.head())\n",
        "\n",
        "# nfc_updated.to_csv(\"NFC_Cleaned.csv\", index=False)\n",
        "\n",
        "\n",
        "# afc = pd.read_csv(\"AFC_Regular_Season_2018-2023.csv\", skiprows=1) ##Removes blank row in header\n",
        "\n",
        "# ##Rename columns to make data fram easier to understand. Yds.1, Yds.2, ... main reasons for adjustment.\n",
        "# afc.rename(columns={\n",
        "#   'Tm': 'Team',\n",
        "#   'Rk': 'Rank',\n",
        "#   'Gtm': 'GameNumber',\n",
        "#   'Pts': 'PointsScored',\n",
        "#   'PtsO': 'PointsAllowed',\n",
        "#   'Cmp': 'PassesCompleted',\n",
        "#   'Att': 'PassesAttempted',\n",
        "#   'Cmp%': 'PercentageCompleted',\n",
        "#   'Yds': 'PassingYards',\n",
        "#   'TD': 'PassingTDs',\n",
        "#   'Y/A': 'PassingYardsPerAttempt',\n",
        "#   'AY/A': 'AdjustedPassingYardsPerAttempt',\n",
        "#   'Rate': 'PasserRating',\n",
        "#   'Sk': 'SacksTaken',\n",
        "#   'Yds.1': 'SackYardsLost',\n",
        "#   'Att.1': 'RushingAttempts',\n",
        "#   'Yds.2': 'RushingYards',\n",
        "#   'TD.1': 'RushingTDs',\n",
        "#   'Y/A.1': 'RushingYardsPerAttempt',\n",
        "#   'Ply': 'OffensivePlays',\n",
        "#   'Tot': 'TotalYards',\n",
        "#   'Y/P': 'YardsPerPlay',\n",
        "#   'FGA': 'FieldGoalAttempts',\n",
        "#   'FGM': 'FieldGoalsMade',\n",
        "#   'XPA': 'ExtraPointAttempts',\n",
        "#   'XPM': 'ExtraPointsMade',\n",
        "#   'Pnt': 'TimesPunted',\n",
        "#   'Yds.3': 'PuntYards',\n",
        "#   'Pass': 'FirstDownsByPassing',\n",
        "#   'Rsh': 'FirstDownsByRushing',\n",
        "#   'Pen': 'FirstDownsByPenalty',\n",
        "#   '1stD': 'FirstDowns',\n",
        "#   '3DConv': 'ThirdDownConversions',\n",
        "#   '3DAtt': 'ThirdDownAttempts',\n",
        "#   '4DConv': 'FourthDownConversions',\n",
        "#   '4DAtt': 'FourthDownAttempts',\n",
        "#   'Pen.1': 'PenaltiesCommitted',\n",
        "#   'Yds.4': 'PenaltyYards',\n",
        "#   'FL': 'FumblesLost',\n",
        "#   'Int': 'InterceptionsThrown',\n",
        "#   'TO': 'TurnoversLost',\n",
        "#   'ToP': 'TimeOfPossession'\n",
        "# }, inplace=True)\n",
        "\n",
        "# afc_updated = afc.copy() ##Work is being done on afc_updated through a copy of afc df\n",
        "# afc_updated['Win'] = afc_updated['Rslt'].apply(lambda x: 1 if x == 'W' else 0) ##changed rslt to win column and represented with binary (win = 1, loss, tie = 0)\n",
        "# afc_updated['Date'] = pd.to_datetime(afc_updated['Date']) ##convert to datetime object\n",
        "# afc_updated['Season'] = afc_updated['Date'].apply(lambda x: x.year - 1 if x.month < 2 else x.year) ##create 'Season' column based on datetime object\n",
        "# afc_updated.drop(columns=['Rank', 'GameNumber', 'Home/Away', 'Opp', 'Rslt', 'Week', 'Date', 'Day', 'OT', 'PassesCompleted', 'PassesAttempted', 'PassingYardsPerAttempt', 'AdjustedPassingYardsPerAttempt', 'RushingYardsPerAttempt', 'FirstDownsByPassing', 'FirstDownsByRushing', 'FirstDownsByPenalty', 'TimeOfPossession'], inplace=True)\n",
        "# ##dropped columns that are not numeric or those that are expected to be non-impactful compared to the remaining.\n",
        "\n",
        "# print(afc_updated.columns)\n",
        "# print(afc_updated.head())\n",
        "\n",
        "# afc_updated.to_csv(\"AFC_Cleaned.csv\", index=False)\n",
        "\n",
        "\n",
        "# nfl_standings = pd.read_csv(\"NFL_Standings_2018-2023.csv\")\n",
        "\n",
        "# nfl_standings.rename(columns={\n",
        "#   'Tm': 'Team',\n",
        "#   'W': 'Wins',\n",
        "#   'L': 'Losses',\n",
        "#   'T': 'Ties',\n",
        "#   'W-L%': 'Win-Loss%',\n",
        "#   'PF': 'PointsScored',\n",
        "#   'PA': 'PointsAllowed',\n",
        "#   'PD': 'PointDifferential',\n",
        "#   'MoV': 'MarginOfVictory',\n",
        "#   'SoS': 'StrengthOfSchedule',\n",
        "#   'SRS': 'TeamRating',\n",
        "#   'OSRS': 'OffenseRating',\n",
        "#   'DSRS': 'DefenseRating',\n",
        "#   'Div': 'Division',\n",
        "# }, inplace=True)\n",
        "\n",
        "# nfl_standings_updated = nfl_standings.copy()\n",
        "# nfl_standings_updated.drop(columns=['Wins', 'Losses', 'Ties', 'Win-Loss%', 'PointsScored', 'PointsAllowed', 'PointDifferential', 'Division'], inplace=True)\n",
        "# print(nfl_standings_updated.columns)\n",
        "# print(nfl_standings_updated.head())\n",
        "\n",
        "\n",
        "# nfl_standings_updated.to_csv(\"NFL_Standings_Cleaned.csv\", index=False)\n",
        "\n",
        "\n",
        "# afc = pd.read_csv(\"AFC_Cleaned.csv\")\n",
        "# nfc = pd.read_csv(\"NFC_Cleaned.csv\")\n",
        "\n",
        "# nfl = pd.concat([afc, nfc], ignore_index=True) #imported clean afc and nfc csvs, merged together into nfl.\n",
        "\n",
        "# print(nfl['Team'].nunique()) #quick double check on # of nfl teams identified - should be 32.\n",
        "# print(nfl['Season'].nunique()) #quick double check on # of nfl seasons identified - should be 6 (2018-2023).\n",
        "\n",
        "# nfl_summary = nfl.groupby(['Team', 'Season']).agg({\n",
        "#   'PointsScored': 'sum',\n",
        "#   'PointsAllowed': 'sum',\n",
        "#   'PercentageCompleted': 'mean',\n",
        "#   'PassingYards': 'sum',\n",
        "#   'PassingTDs': 'sum',\n",
        "#   'PasserRating': 'mean',\n",
        "#   'SacksTaken': 'sum',\n",
        "#   'SackYardsLost': 'sum',\n",
        "#   'RushingAttempts': 'sum',\n",
        "#   'RushingYards': 'sum',\n",
        "#   'RushingTDs': 'sum',\n",
        "#   'OffensivePlays': 'sum',\n",
        "#   'TotalYards': 'sum',\n",
        "#   'YardsPerPlay': 'mean',\n",
        "#   'FieldGoalAttempts': 'sum',\n",
        "#   'FieldGoalsMade': 'sum',\n",
        "#   'ExtraPointAttempts': 'sum',\n",
        "#   'ExtraPointsMade': 'sum',\n",
        "#   'TimesPunted': 'sum',\n",
        "#   'PuntYards': 'sum',\n",
        "#   'FirstDowns': 'sum',\n",
        "#   'ThirdDownConversions': 'sum',\n",
        "#   'ThirdDownAttempts': 'sum',\n",
        "#   'FourthDownConversions': 'sum',\n",
        "#   'FourthDownAttempts': 'sum',\n",
        "#   'PenaltiesCommitted': 'sum',\n",
        "#   'PenaltyYards': 'sum',\n",
        "#   'FumblesLost': 'sum',\n",
        "#   'InterceptionsThrown': 'sum',\n",
        "#   'TurnoversLost': 'sum',\n",
        "#   'Win': 'sum'\n",
        "# }).reset_index()\n",
        "# ##Group by Team and Season, sum most stats and use the mean for the stats that require it to be meaningful.\n",
        "\n",
        "# nfl_summary = nfl_summary.round(2) ##had some crazy long decimals, cutting means off at 2 decimal places.\n",
        "# nfl_summary.to_csv(\"NFL_Summary.csv\", index=False)\n",
        "\n",
        "# colts_2023 = pd.read_csv(\"Colts_2023_Append.csv\")\n",
        "# afc_cleaned = pd.read_csv(\"AFC_Cleaned.csv\")\n",
        "# afc_cleaned = pd.concat([afc_cleaned, colts_2023], ignore_index=True)\n",
        "\n",
        "# afc_cleaned.to_csv(\"AFC_Cleaned.csv\", index=False)\n",
        "\n",
        "\n",
        "# nfl_data_one = pd.read_csv(\"NFL_Summary.csv\")\n",
        "# nfl_data_two = pd.read_csv(\"NFL_Standings_Cleaned.csv\")\n",
        "\n",
        "# nfl_data_two['Team']= nfl_data_two['Team'].str.replace('[*+]', '', regex=True)\n",
        "\n",
        "# print(nfl_data_two.columns)\n",
        "# print(nfl_data_one.columns)\n",
        "\n",
        "# nfl_data_final = pd.merge(\n",
        "#   nfl_data_one,\n",
        "#   nfl_data_two,\n",
        "#   on=['Team', 'Season'],\n",
        "#   how='inner'\n",
        "# )\n",
        "\n",
        "# nfl_data_final.to_csv(\"NFL_Data.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b3d8846",
      "metadata": {
        "id": "8b3d8846"
      },
      "source": [
        "## Exploratory Data Analysis\n",
        "\n",
        "In this section, we explore the relationships between regular season performance metrics and playoff appearances from 2018 to 2023. The goal is to identify patterns and key features that differ between teams that made the playoffs and those that did not.\n",
        "\n",
        "### Team Rating vs Strength of Schedule\n",
        "The scatterplot below shows the relationship between a team's rating (overall performance) and their strength of schedule. Playoff teams tend to have higher overall ratings, regardless of schedule difficulty.\n",
        "\n",
        "Green = Made Playoffs\n",
        "\n",
        "Red = Missed Playoffs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9edfb47d",
      "metadata": {
        "id": "9edfb47d"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(\n",
        "  x='TeamRating',\n",
        "  y='StrengthOfSchedule',\n",
        "  hue='MadePlayoffs',\n",
        "  palette={0: 'red', 1: 'green'},\n",
        "  data=df,\n",
        "  alpha=0.7\n",
        ")\n",
        "\n",
        "plt.title('Team Rating vs Strength of Schedule by Playoff Status')\n",
        "plt.ylabel('Strength of Schedule')\n",
        "plt.xlabel('Team Rating')\n",
        "plt.ylim(-3, 3)\n",
        "\n",
        "handles, labels = plt.gca().get_legend_handles_labels()\n",
        "labels = ['Missed', 'Made']\n",
        "plt.legend(handles, labels, title=\"Playoffs\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f10bcace",
      "metadata": {
        "id": "f10bcace"
      },
      "source": [
        "### Correlation with Playoff Apperances.\n",
        "\n",
        "To understand which features are most impactful, we computed a correlation matrix using numeric variables. Below are the features most positively and negatively correlated with making the playoffs.\n",
        "\n",
        "This helps identify which regular season statistics are most predictive of postseason success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aebefbaa",
      "metadata": {
        "id": "aebefbaa"
      },
      "outputs": [],
      "source": [
        "# Using a correlation matrix to determine which of the variables are most impactful on making the playoffs.\n",
        "# Set the original dataframe to a numbers-only dataframe.\n",
        "numbers_df = df.select_dtypes(include=[np.number])\n",
        "corr_matrix = numbers_df.corr()\n",
        "corr_matrix.to_csv(\"Correlation_Matrix.csv\")\n",
        "\n",
        "\n",
        "# Removed 'MadePlayoffs' and 'PlayoffWin' rows, displayed top 10 positive correlation values.\n",
        "corr_with_playoffs = corr_matrix['MadePlayoffs'].sort_values(ascending=False)\n",
        "corr_with_playoffs_pos = corr_with_playoffs.sort_values(ascending=False)\n",
        "corr_with_playoffs_pos = corr_with_playoffs_pos.drop(['MadePlayoffs', 'PlayoffWin'], axis=0)\n",
        "print(\"Positive Correlation For Reaching Playoffs:\")\n",
        "print(corr_with_playoffs_pos.head(10))\n",
        "\n",
        "# Displayed top 10 negative correlation values.\n",
        "corr_with_playoffs_neg = corr_with_playoffs.sort_values(ascending=True)\n",
        "print(\"Negative Correlation For Reaching Playoffs:\")\n",
        "print(corr_with_playoffs_neg.head(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e953770",
      "metadata": {
        "id": "6e953770"
      },
      "source": [
        "### Offensive and Defensive Ratings Over Time\n",
        "To better undertand performance trends over time, we visualized the average offensive and defensive ratings for teams that made and missed the playoffs across the 2018-2023 seasons.\n",
        "\n",
        "This shows whether playoff teams consistently outperform non-playoff teams in specific areas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b0f52cf",
      "metadata": {
        "id": "6b0f52cf"
      },
      "outputs": [],
      "source": [
        "# Create two line plots to see the differences in Offense Rating and Defense Rating between Playoff and Non-Playoff teams.\n",
        "\n",
        "df['Season'] = df['Season'].astype(str)\n",
        "\n",
        "df_line_group = df.groupby(['Season', 'MadePlayoffs'])[['OffenseRating', 'DefenseRating']].mean().reset_index()\n",
        "\n",
        "sns.lineplot(data=df_line_group, x='Season', y='DefenseRating', hue='MadePlayoffs', marker='o')\n",
        "plt.title('Defense Rating - Reaching Playoffs')\n",
        "plt.xlabel('Season')\n",
        "plt.ylabel('Defense Rating')\n",
        "plt.ylim(-3, 3)\n",
        "handles, labels = plt.gca().get_legend_handles_labels()  # Grab existing handles and labels for legend\n",
        "labels = ['Missed', 'Made'] # Change labels to missed and made\n",
        "plt.legend(handles, labels, title=\"Playoffs\")\n",
        "\n",
        "plt.show()\n",
        "\n",
        "sns.lineplot(data=df_line_group, x='Season', y='OffenseRating', hue='MadePlayoffs', marker='o')\n",
        "plt.title('Offense Rating - Reaching Playoffs')\n",
        "plt.xlabel('Season')\n",
        "plt.ylabel('Offense Rating')\n",
        "plt.ylim(-5, 5)\n",
        "handles, labels = plt.gca().get_legend_handles_labels()  # Grab existing handles and labels for legend\n",
        "labels = ['Missed', 'Made'] # Change labels to missed and made\n",
        "plt.legend(handles, labels, title=\"Playoffs\")\n",
        "handles, labels = plt.gca().get_legend_handles_labels()  # Grab existing handles and labels for legend\n",
        "labels = ['Missed', 'Made'] # Change labels to missed and made\n",
        "plt.legend(handles, labels, title=\"Playoffs\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6c8efeb",
      "metadata": {
        "id": "e6c8efeb"
      },
      "source": [
        "## Splitting the Dataset\n",
        "\n",
        "- **Training Set:** Used to teach the model patterns in the data.\n",
        "- **Validation Set:** Used to tune hyperparamters (like depth, regularization, etc).\n",
        "- **Test Set:** Used at the end to evaluate how well the final model performs on unseen data.\n",
        "\n",
        "---\n",
        "### Explanation:\n",
        "In this step, we prepared our dataset for machine learning by splitting it into training, validation, and test sets.\n",
        "\n",
        "- Since `Team` and `Season` are categorical variables, we used dummy encoding to convert them into numerical format. This creates new binary columns for each team and each season, allowing machine learning models to interpret these values without introducing any ordinal bias.\n",
        "- After encoding, we dropped the original `Team` and `Season` colummns to avoid redundancy.\n",
        "- We also removed `PlayoffWin`, since it's only relevant after a team has already made the playoffs. Meaning, it would leak future information into our model.\n",
        "- We defined `X` as our input features (regular season stats) and `y` as our target variable (`MadePlayoffs`) which indicates whether a team qualified for the postseason (1) or (0).\n",
        "- The dataset was then split using `train_test_split`:\n",
        "    - First into 80% training+validation and 20% test.\n",
        "    - Then split again into 60% training and 20% validation.\n",
        "- We used stratified sampling to maintain the ratio of playoff vs. non-playoff teams in each split.\n",
        "- A fixed `random_state` ensures that our splits are reporiducible every time we rerun the notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61faf7bd",
      "metadata": {
        "id": "61faf7bd"
      },
      "outputs": [],
      "source": [
        "# Dummy Encode\n",
        "season_dummies = pd.get_dummies(df[\"Season\"], prefix = \"Season\")\n",
        "team_dummies = pd.get_dummies(df[\"Team\"], prefix=\"Team\")\n",
        "df = pd.concat([df, team_dummies, season_dummies], axis=1)\n",
        "df = df.drop(columns=[\"Team\", \"Season\"])\n",
        "\n",
        "# Drop irrelevant columns\n",
        "X = df.drop(columns=[\"MadePlayoffs\", \"PlayoffWin\"])\n",
        "y = df[\"MadePlayoffs\"]\n",
        "\n",
        "# Split and train (80%) and test (20%)\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Split train_val into train (60%) and validation (20%)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42\n",
        ")\n",
        "\n",
        "print(\"Training set:\", X_train.shape)\n",
        "print(\"Validation set:\", X_val.shape)\n",
        "print(\"Test set:\", X_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8c4df79",
      "metadata": {
        "id": "c8c4df79"
      },
      "source": [
        "## Feature Preparation\n",
        "In this step, we prepared our feature set for modeling by scaling all numerical input features. We used `StandardScaler`, which standarizes each feature by removing the mean and scaling to unit variance. (mean = 0, standard deviation = 1).\n",
        "\n",
        "This step is especially important for our Logiistic Regression model, which is sensitive to feature scale. Without scaling, features with large values (i.e. total yards) could disproportionately influnece the model.\n",
        "\n",
        "Scaling was performed as follows:\n",
        "- The scaler was fitted only on the training set to prevent data leakage.\n",
        "- The same tranformation was then applied to the validation and test sets.\n",
        "\n",
        "For the Random Forest model, feature scaling is not necessary, since tree based models are invariant to feature scale. Therefore, we also kept a copy of the unscaled feature sets specifically for the decision tree model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7398d9d0",
      "metadata": {
        "id": "7398d9d0"
      },
      "outputs": [],
      "source": [
        "#training, validation, test data (for decision tree)\n",
        "X_train_scaled = X_train.copy()\n",
        "X_val_scaled =  X_val.copy()\n",
        "X_test_scaled = X_test.copy()\n",
        "\n",
        "#Initialize scaler and fit on training data\n",
        "scaler = StandardScaler()\n",
        "scaler.fit(X_train)\n",
        "\n",
        "# Apply scaling (For linear regression)\n",
        "X_train_scaled = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
        "X_val_scaled = pd.DataFrame(scaler.transform(X_val), columns=X_val.columns)\n",
        "X_test_scaled = pd.DataFrame(scaler.transform(X_test), columns=X_test.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "845ffae3",
      "metadata": {
        "id": "845ffae3"
      },
      "source": [
        "## Logistic Regression Model\n",
        "We trained a Logistic Regresio model using the scaled training data. Logistic regression is a linear model that predicts the probability that a team made the playoffs (`MadePlayoffs = 1`) based on its regular season stats.\n",
        "\n",
        "This model is sensitive to the scale of features, which is why we applied `StandardScalar` earlier. After training, we used the test set to evaluate the model's performance.\n",
        "\n",
        "### Evaluation Metrics:\n",
        "- **Accuracy** measures the overall correctness of the model.\n",
        "- **Precision** shows how accurate the model is when it predicts a team will make the playoffs.\n",
        "- **Recall** shows how many actual playoff teams were correctly indentified.\n",
        "- **F1-Score** is the balance of precision and recall.\n",
        "- **Confusion Matrix** gives a breakdown of correct vs. incorrect predictions:\n",
        "    - True Positives: predictived playoffs and made playoffs\n",
        "    - True Negatives: predicted no playoffs and didn't make playoffs\n",
        "    - False Positives: predicted playoffs, but didn't make it.\n",
        "    - False Negatives: Predicted no playoffs, actually made it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b0d3818",
      "metadata": {
        "id": "7b0d3818"
      },
      "outputs": [],
      "source": [
        "# Create Logistic Regression Model with scaled X_train and y_train.\n",
        "reg_model = LogisticRegression()\n",
        "\n",
        "reg_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Predict target variable based on scaled X_test.\n",
        "y_pred = reg_model.predict(X_test_scaled)\n",
        "\n",
        "# Accuracy score for y_pred, 84.6%.\n",
        "reg_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Accuracy Score: \", reg_accuracy)\n",
        "\n",
        "# Confusion Matrix for y_pred with 19 TN, 4 FP, 2 FN, 14 TP.\n",
        "reg_conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Logistic Regression Confusion Matrix:\\n\", reg_conf_matrix)\n",
        "\n",
        "# Precision - accuracy in predicting a team will make the playoffs.\n",
        "reg_precision = precision_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Precision Score: \", reg_precision)\n",
        "\n",
        "# Recall - accuracy in predicting a team will miss the playoffs.\n",
        "reg_recall = recall_score(y_test, y_pred)\n",
        "print(\"Logistic Regression Recall Score: \", reg_recall)\n",
        "\n",
        "# F1-Score - combination of precision and recall to help understand the overall model's effectiveness.\n",
        "reg_f1 = f1_score(y_test, y_pred)\n",
        "print(\"Logistic Regression F1-Score: \", reg_f1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0181b3bf",
      "metadata": {
        "id": "0181b3bf"
      },
      "source": [
        "These results show that the model performs well overall, with a strong balance between precision and recall, especialy in identifying playoff teams. (recall = 87.5%)\n",
        "\n",
        "\n",
        "### Confusion Matrix (Logistic Regression)\n",
        "Below we have a plot for the confusion matrix for logistic regression. It shows that it correctly classified 19 non-playoff teams, and 14 playoff teams. There were 4 false positives and 2 false negatives. This suggests that the model is balanced and performs well at both identifying successful and unsuccessful teams.\n",
        "\n",
        "Overall, this model is pretty strong with 33 out of 39 predictions being correct.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20e11227",
      "metadata": {
        "id": "20e11227"
      },
      "outputs": [],
      "source": [
        "# Graphing the confusion matrix\n",
        "ConfusionMatrixDisplay.from_estimator(\n",
        "    reg_model,\n",
        "    X_test_scaled,\n",
        "    y_test,\n",
        "    cmap='BuPu',\n",
        "    colorbar=True\n",
        ")\n",
        "plt.title(\"Logistic Regression - Confusion Matrix\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52531b21",
      "metadata": {
        "id": "52531b21"
      },
      "source": [
        "## Random Forest Classifier\n",
        "We also trained a Random Forest Classifier to predict whether a team made the playoffs based on regular season statistics. Random forests are ensemble models tht work by building multiple decision trees and averaging their predictions, which make them less prone to overfitting than a single decision tree.\n",
        "\n",
        "Unlike logic regression, random forests:\n",
        "- Handle nonlinear relationships\n",
        "- Don't require feature scaling\n",
        "- Provide feature importance scores\n",
        "\n",
        "We used the unscaled features for this model, since tree based models aren't affected by different feature magnitudes.\n",
        "\n",
        "We evaluated the same metrics as before. (Accuracy, Precision, Recall, F1-Score, Confusion Matrix).\n",
        "This allows for a direct comparison between the random forest and logistic regression models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "701a5c54",
      "metadata": {
        "id": "701a5c54"
      },
      "outputs": [],
      "source": [
        "# Initialize Model\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=100,\n",
        "    max_depth=None,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the validation set\n",
        "y_val_pred = rf_model.predict(X_val)\n",
        "\n",
        "#Print eval\n",
        "print(\"Validation Accurary:\", accuracy_score(y_val, y_val_pred))\n",
        "print(\"\\nClassification Report: \\n\", classification_report(y_val, y_val_pred))\n",
        "print(\"\\nConfusion Matrix: \\n\", confusion_matrix(y_val, y_val_pred))\n",
        "\n",
        "importances = rf_model.feature_importances_\n",
        "features = X_train.columns\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbdc2d9d",
      "metadata": {
        "id": "dbdc2d9d"
      },
      "source": [
        "The chart below displays the top 10 most important features used by the Random Forest Classifier to predict playoff qualification. These features had the greatest impact on the model's decision making process.\n",
        "\n",
        "- The model assigns higher and importance to features that are frequentlky used in its decision trees to split the data\n",
        "- Features like TeamRating, MarginOfVictory. and Offensive metrics such as TotalYards and PasserRating appear at the tp, indicating strong influence on playoff outcomes.\n",
        "- Understanding feature importance helps us identify which regular season stats are most critical for predicting postseason success."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe87939",
      "metadata": {
        "id": "2fe87939"
      },
      "outputs": [],
      "source": [
        "# Plot top 10 features\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.barh(\n",
        "    y=features[sorted_indices],\n",
        "    width=importances[sorted_indices],\n",
        "    color=\"#71c7ec\"  # pastel blue\n",
        ")\n",
        "plt.xlabel(\"Feature Importance\")\n",
        "plt.title(\"Top 10 Most Important Features - Random Forest\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "382aea58",
      "metadata": {
        "id": "382aea58"
      },
      "source": [
        "Additionally, here is a side by side of both models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb963813",
      "metadata": {
        "id": "fb963813"
      },
      "outputs": [],
      "source": [
        "rf_accuracy = accuracy_score(y_val, y_val_pred)\n",
        "rf_precision = precision_score(y_val, y_val_pred)\n",
        "rf_recall = recall_score(y_val, y_val_pred)\n",
        "rf_f1 = f1_score(y_val, y_val_pred)\n",
        "rf_cm = confusion_matrix(y_val, y_val_pred)\n",
        "\n",
        "print(\"Random Forest Accuracy Score: \", rf_accuracy)\n",
        "print(\"Logistic Regression Accuracy Score: \", reg_accuracy)\n",
        "print(\"  \" )\n",
        "print(\"Random Forest Confusion Matrix:\\n\", rf_cm)\n",
        "print(\"Logistic Regression Confusion Matrix:\\n\", reg_conf_matrix)\n",
        "print(\"  \" )\n",
        "print(\"Random Forest Precision Score: \", rf_precision)\n",
        "print(\"Random Forest Recall Score:: \", rf_recall)\n",
        "print(\"Random Forest F1-Score: \", rf_f1)\n",
        "\n",
        "# Bar chart comparing models\n",
        "model_names = ['Logistic Regression', 'Random Forest']\n",
        "accuracy_scores = [reg_accuracy, rf_accuracy]\n",
        "f1_scores = [reg_f1, rf_f1]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(x - width/2, accuracy_scores, width, label='Accuracy', color='#71c7ec')   # pastel blue\n",
        "ax.bar(x + width/2, f1_scores, width, label='F1 Score', color='#b0e0a8')         # pastel green\n",
        "\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_ylim(0, 1)  # optional: set consistent y-axis\n",
        "ax.set_title('Model Comparison: Accuracy and F1 Score')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(model_names)\n",
        "ax.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f56080",
      "metadata": {
        "id": "c9f56080"
      },
      "source": [
        "These results suggest that the random forest model slightly outperforms logisitic regression in overall accuracy and precision, while logistic regression has a slightly better recall. Both models are well-balanced and show strong predictive capabitlity."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f5ab3e9",
      "metadata": {
        "id": "3f5ab3e9"
      },
      "source": [
        "## Hyperparameter Tuning - Logitic Regression\n",
        "To improve our logistic regression model, we performed hyperparamter tuning using `GridSearchCV`. This technique tests mulitple combinations of parameters to identify the best performing setup using cross validation.\n",
        "\n",
        "We tested the following hyperparameters:\n",
        "- `C`: Controls regularization strength (smaller = more regularization).\n",
        "- `penalty`: Type of regularization used. We tested `l2`.\n",
        "- `solver`: Optimization algorithm used ('liblinear' and 'lbfgs').\n",
        "- `max_iter`: Maximum number of iterations to allow convergence.\n",
        "- `tol`: Tolerance for stopping criteria.\n",
        "- `class_weight`: Balanced class weights to handle uneven class distribution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71760d85",
      "metadata": {
        "id": "71760d85"
      },
      "outputs": [],
      "source": [
        "log_reg_tuning = LogisticRegression()\n",
        "\n",
        "param_grid = {\n",
        "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "    'penalty': ['l2'],\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'max_iter': [2000, 3000, 5000],\n",
        "    'tol': [1e-4, 1e-5, 1e-6],\n",
        "    'class_weight': ['balanced']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=log_reg_tuning, param_grid=param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train_scaled, y_train)\n",
        "\n",
        "optimal_log_reg_model = grid_search.best_estimator_\n",
        "\n",
        "test_accuracy = optimal_log_reg_model.score(X_test_scaled, y_test)\n",
        "test_accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99633bc7",
      "metadata": {
        "id": "99633bc7"
      },
      "source": [
        "The Hyperparamter tuning significantly improved the model's ability to generalize to unseen data. By fine tuning the solver behavior and regularization strength, we gained a more optimized and reliable model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada42e97",
      "metadata": {
        "id": "ada42e97"
      },
      "source": [
        "## Challenges\n",
        "\n",
        "### Missing Team-Year Data\n",
        "- While aggregating the full dataset, we discovered that the 2023 season for teh Indianapolis Colts was missing. To correct this, we created a supplemental file (`Colts_2023_Append.csv`) and appended it manually.\n",
        "\n",
        "### Inconsistent Team Name Formatting\n",
        "During the merge with NFL standings data, we found that some team names contained characters like `*` and `+` (e.g., `Chargers*`). These needed to be removed with a regular expression to ensure a successful merge on `Team` and `Season`.\n",
        "\n",
        "### Model Runtime Issues During GridSearch\n",
        "When tuning our logistic regression model, some solvers took longer to converge or produced convergence warnings. We fixed this by:\n",
        "- Increasing `max_iter` to allow more convergence time\n",
        "- Reducing the number of solvers tested\n",
        "- Using only the scaled training set for tuning\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1284a2eb",
      "metadata": {
        "id": "1284a2eb"
      },
      "source": [
        "## Final Evaluation and Conclusion\n",
        "\n",
        "After training and evaluating two different models, we compared their performance based on accuracy, precision, recall and F1-score.\n",
        "\n",
        "- **Logistic Regression** performed slightly better on the test set accuracy after tuning, showing that regularization and proper hyperparameter selection helped improve the model.\n",
        "- ** Random Forest had the highest precision and F1-score, meaning it was very effective at reducing false positives while maintaining strong overall prediction quality.\n",
        "- Both models had a good balance of Recall, meaning they successfully identified most of the actual playoff teams.\n",
        "\n",
        "This Project shows that it is possible to accurately predict NFL playoff appearances using regular season team statistics. Both models performed well, but Logistic Regression showed the most improvement after tuning. This highlights the value of hyperparamter optimization and careful model selection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce024729",
      "metadata": {
        "id": "ce024729"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}